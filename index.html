<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="MasQCLIP, Open-Vocabulary Universal Image Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MasQCLIP for Open-Vocabulary Universal Image Segmentation</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EH539NRBEM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-EH539NRBEM');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MasQCLIP for Open-Vocabulary Universal Image Segmentation<br>
              <small>ICCV 2023</small>
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="mailto:xinx8@illinois.edu">Xin Xu<sup>1</sup>*</a>,</span>
              <span class="author-block">
                <a href="https://tyxiong23.github.io/">Tianyi Xiong<sup>2</sup>*</a>,</span>
              <span class="author-block">
                <a href="">Zheng Ding<sup>3</sup></a>,</span>
              <span class="author-block">
                <a href="https://pages.ucsd.edu/~ztu/">Zhuowen Tu<sup>3</sup></a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Peking University,
              </span>
              <span class="author-block">
                <sup>2</sup>Tsinghua University,
              </span>
              <span class="author-block">
                <sup>3</sup>UC San Diego
              </span>
            </div>

            <div class="is-size-6 publication-authors" style="color: gray">
              <span class="author-block">
                *Equal Contribution
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MasQCLIP_for_Open-Vocabulary_Universal_Image_Segmentation_ICCV_2023_paper.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="./static/files/poster_iccv2023_masqclip_latest.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/mlpc-ucsd/MasQCLIP" class="button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>


            <div class="columns is-centered has-text-centered">
              <!-- <div class="column is-7"> -->
              <!-- <h2 class="title is-3">Main Results</h2> -->
              <!-- <div class="content has-text-justified" style="font-size:20px;">
                  With a unified framework, MasQCLIP achieves substantial performance gain across all three open-vocabulary segmentation tasks.
                </div> -->
              <img src="./static/images/ov_example.png" style="width: 85%; margin-top: 15px; margin-bottom: 0px;" />
              <!-- </div> -->
            </div>
            <div class="columns is-centered has-text-centered"><p style="font-size: 20px;">MasQCLIP exhibits the ability to segment objects of arbitrary classes as per user specifications and to
              discriminate between subtle distinctions within them.</p></div>
            

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-11">
            <h2 class="title is-3">Abstract</h2>
            <!-- <hr> -->
            <div class="content has-text-justified" style="font-size:20px;">
              <p>
                We present a new method for open-vocabulary universal image segmentation, which is capable of performing
                instance, semantic, and panoptic segmentation under a unified framework. Our approach, called MasQCLIP,
                seamlessly integrates with a pre-trained CLIP model by utilizing its dense features, thereby
                circumventing the need for extensive parameter training.
              </p>
              <p>
                MasQCLIP emphasizes two new aspects when building an image segmentation method with a CLIP model:
                (1) a student-teacher module to deal with masks of the novel (unseen) classes by distilling information
                from the base (seen) classes; (2) a fine-tuning
                process to update model parameters for the queries \(Q\) within the CLIP model. Thanks to these two
                simple
                and intuitive designs, MasQCLIP is able to achieve state-of-the-art performances with a substantial gain
                over the competing methods by a large margin across all three tasks, including open-vocabulary instance,
                semantic, and panoptic segmentation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">

        <div class="column is-7">
          <h2 class="title is-3">Method</h2>
          <hr>
          <img src="./static/images/main.png" class="interpolation-image" alt="Interpolate start reference image." />
          <div class="content has-text-justified" style="font-size:20px;">
            MasQCLIP consists of a class-agnostic mask proposal network and a mask classification module based on CLIP.
            In the mask proposal network, we apply <b><i>progressive distillation</i></b> to segment masks beyond base
            classes.
            After we obtain an open-world mask proposal network, the predicted masks are then sent to the classification
            module to obtain labels.
            To efficiently utilize the dense CLIP features, we propose <b><i>MasQ-Tuning</i></b>.
            We set new query projections \(f_{Q}^{\prime}\) for the Mask Class Tokens to obtain optimal attention
            weights, and \(f_Q^\prime\) at each layer are the only learnable parameters.
          </div>
        </div>
      </div>
      <div class="container is-max-desktop">

        <div class="columns is-centered">

          <div class="column">
            <div class="content">
              <h3 class="title is-4">Progressive Distillation</h3>
              <!-- <p  style="font-size:16px;">
              By utilizing the object score as an indicator of mask quality, 
              we can filter high-quality mask proposals that do not overlap with mask annotations of base categories, 
              thus producing extra annotations for training.
            </p> -->
              <img src="./static/images/progressive-distillation.png" class="interpolation-image"
                alt="Interpolate start reference image." />
            </div>
          </div>

          <div class="column">
            <h3 class="title is-4">MasQ-Tuning</h3>
            <div class="columns is-centered">
              <div class="column content">
                <p style="font-size:19px;">
                  To enhance adaptation from image classification to mask classification while maintaining the
                  generalization ability of CLIP, we apply new query projections \(f_Q^\prime\) to each cross-attention
                  layer for Mask Class Tokens, i.e.
                </p>
                <p>
                  $$\text{CrossAttn}(\cdot) = \text{softmax}(\mathbf{Q}_{\text{mask}}^\prime K_{\text{img}}^T +
                  \mathcal{M}_{\text{mask}}) \cdot V_{\text{img}}$$
                </p>
                <p>
                  $$\mathbf{Q}_{\text{mask}}^\prime, K_{\text{img}}, V_{\text{img}} =
                  \mathbf{f_Q^\prime}(x_\text{mask}), f_K(x_\text{img}), f_V(x_\text{img})$$
                </p>
              </div>

            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">

        <div class="column is-7">
          <h2 class="title is-3">Main Results</h2>
          <hr>
          <img src="./static/images/teaser.png" class="interpolation-image" alt="Interpolate start reference image." />
          <!-- <div class="content has-text-justified" style="font-size:20px;">
            MasQCLIP consists of a class-agnostic mask proposal network and a mask classification module based on CLIP.
            In the mask proposal network, we apply <b><i>progressive distillation</i></b> to segment masks beyond base
            classes.
            After we obtain an open-world mask proposal network, the predicted masks are then sent to the classification
            module to obtain labels.
            To efficiently utilize the dense CLIP features, we propose <b><i>MasQ-Tuning</i></b>.
            We set new query projections \(f_{Q}^{\prime}\) for the Mask Class Tokens to obtain optimal attention
            weights, and \(f_Q^\prime\) at each layer are the only learnable parameters.
          </div> -->
        </div>
      </div>
  </section>



  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-7">
          <h2 class="title is-3">Qualitative Results</h2>
          <hr>
          <!-- <div class="content has-text-justified" style="font-size:20px;">
        With a unified framework, MasQCLIP achieves substantial performance gain across all three open-vocabulary segmentation tasks.
      </div> -->
          <p style="font-size: 23px; font-weight: bold; margin-top: 10px;">Semantic Segmentation</p>
          <img src="./static/images/sem_seg_vis.png" />
          <p style="font-size: 23px; font-weight: bold; margin-top: 10px;">Instance Segmentation</p>
          <img src="./static/images/ins_seg_vis.png" />
          <p style="font-size: 23px; font-weight: bold; margin-top: 10px;">Panoptic Segmentation</p>
          <img src="./static/images/pan_seg_vis.png" />
        </div>
      </div>
    </div>
  </section>

  

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{xu2023masqclip,
      author    = {Xu, Xin and Xiong, Tianyi and Ding, Zheng and Tu, Zhuowen},
      title     = {MasQCLIP for Open-Vocabulary Universal Image Segmentation},
      booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
      month     = {October},
      year      = {2023},
      pages     = {887-898}
}</code></pre>
  </div>
</section>


  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This website's source code is borrowed from the <a href="https://nerfies.github.io">Nerfies</a> project
              page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
